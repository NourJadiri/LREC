\section{Results}

In this section, we present the empirical results of our experiments. We first demonstrate the competitive performance of the Agora framework through official SemEval-2025 rankings. We then provide a detailed architectural comparison to understand the source of Agora's success, and conclude with an analysis of its voting strategies.

\subsection{State-of-the-Art Performance in SemEval-2025}

The ultimate validation of the Agora framework comes from its performance in the official SemEval-2025 Task 10 competition. Our submitted system, which was the Agora multi-agent ensemble, achieved top-tier rankings across all five languages, including a first-place finish in Hindi.

\begin{table}[ht]
\centering
\caption{Performance of the Agora Framework in SemEval-2025 Task 10 across Languages.}
\label{tab:semeval_rankings}
\begin{tabular}{lc}
\hline
\textbf{Language} & \textbf{Rank} \\
\hline
Hindi & \textbf{1st} \\
English & 2nd \\
Portuguese & 2nd \\
Russian & 3rd \\
Bulgarian & 4th \\
\hline
\end{tabular}
\end{table}

These results confirm that the consensus-based approach is not just a theoretical improvement but a practical, state-of-the-art method for complex HMLC tasks. Achieving a first-place and two second-place finishes in a highly competitive shared task validates the robustness and generalizability of the Agora architecture.

Notably, these rankings represent a substantial improvement over our earlier submission using the Actor-Critic pipeline approach. Table~\ref{tab:actor_critic_rankings} shows the performance of that intermediate architecture.

\begin{table}[ht]
\centering
\caption{Performance of the Actor-Critic Pipeline in SemEval-2025 Task 10 across Languages.}
\label{tab:actor_critic_rankings}
\begin{tabular}{lc}
\hline
\textbf{Language} & \textbf{Rank} \\
\hline
English & 2nd \\
Portuguese & 2nd \\
Hindi & 3rd \\
Russian & 4th \\
Bulgarian & 5th \\
\hline
\end{tabular}
\end{table}

While the Actor-Critic approach already achieved competitive results with consistent top-5 rankings, the transition to Agora's multi-agent ensemble brought further improvements, most notably elevating Hindi from 3rd to 1st place. This progression demonstrates the value of moving from a single-critic refinement paradigm to a consensus-based ensemble approach.

\subsection{Architectural Comparison and Ablation Study}

To understand the source of Agora's success, we conducted a controlled comparison on the English dataset between our final Agora system, an intermediate Actor-Critic pipeline, and a Naive Baseline. The results are presented in Table~\ref{tab:main_comparison}.

\begin{table*}[ht]
\centering
\caption{Main Performance Comparison on the English Dataset (F1-Samples).}
\label{tab:main_comparison}
\begin{tabular}{lcc}
\hline
\textbf{System Configuration} & \textbf{F1-Samples Score} & \textbf{Improvement over Baseline} \\
\hline
1. Naive Baseline (Single-Pass) & 0.382 & --- \\
2. Actor-Critic Pipeline & 0.417 & +0.035 \\
3. Agora (3-Agent Intersection) & \textbf{0.424} & \textbf{+0.042} \\
\hline
\end{tabular}
\end{table*}

This ablation study reveals a clear performance pathway. The Naive Baseline represents the H3Prompting approach described in Section~\ref{sec:baseline}: a single agent per level, applying one LLM call per hierarchical step without refinement. This baseline achieves an F1-Samples score of 0.382.

The Actor-Critic Pipeline provides a significant improvement of +0.035, demonstrating that a self-refinement loop can effectively correct errors and improve classification quality.

However, our proposed Agora framework delivers an even greater performance gain. It achieves the highest F1-Samples score of 0.424, a +0.042 point improvement (+11.0\%) over the Naive Baseline. This result confirms our central hypothesis: while single-agent refinement is beneficial, the consensus-based approach of a multi-agent ensemble is a more robust and superior method for enhancing LLM reliability.

\subsection{Analysis of Agora's Voting Strategies}

To isolate the effect of the ensemble itself, we compared a single-agent Agora configuration (``Vanilla'') against the different multi-agent voting strategies. Table~\ref{tab:voting_strategies} shows that the choice of aggregation method is critical to the ensemble's success.

\begin{table*}[!ht]
\centering
\caption{Ablation of Agora Configurations on the English Dataset (F1-Samples).}
\label{tab:voting_strategies}
\begin{tabular}{lcc}
\hline
\textbf{Agora Configuration} & \textbf{F1-Samples Score} & \textbf{Improvement over Single Agent} \\
\hline
1. Single Agent (Vanilla) & 0.389 & --- \\
2. 3-Agent Union & 0.375 & -0.014 \\
3. 3-Agent Majority Vote & 0.402 & +0.013 \\
4. 3-Agent Intersection & \textbf{0.424} & \textbf{+0.035} \\
\hline
\end{tabular}
\end{table*}

This analysis provides a crucial insight: simply combining agent outputs is not enough. The Union strategy, which naively accepts all proposed labels, actually performs worse than a single agent by aggregating noise. In contrast, the consensus-based mechanisms deliver clear benefits. Both Majority Vote (+0.013) and Intersection (+0.035) significantly outperform the single agent. The Intersection strategy, requiring unanimous agreement, proves most effective for this task, successfully filtering out stochastic, low-confidence predictions to achieve the best overall performance. This demonstrates that the core advantage of Agora lies in its ability to systematically reduce noise through consensus.

