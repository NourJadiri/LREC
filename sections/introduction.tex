\section{Introduction}

Text Classification (TC) is a foundational task in Natural Language Processing (NLP) \citep{Zangari2024}. While traditional approaches often model TC as a single-label problem, real-world texts frequently contain multiple overlapping themes, motivating the use of Multi-Label Classification (MLC) \citep{Hu2025,TidakeSane2018}. A particularly challenging yet crucial variant is Hierarchical Multi-Label Classification (HMLC), where labels are organized in a predefined hierarchy (e.g., a tree or DAG) \citep{liu2023recentadvanceshierarchicalmultilabel}. This structure is common in domains requiring nuanced analysis, such as misinformation detection, where identifying nested propaganda narratives is a key challenge.

The advent of Large Language Models (LLMs) has opened new frontiers for HMLC, enabling powerful zero-shot classification without extensive labeled data. However, this power comes with significant reliability challenges. LLMs are known to be stochastic, producing different outputs for the same input, and often exhibit low instruction fidelity, failing to consistently adhere to complex hierarchical constraints or output formats \citep{some_citation_on_llm_brittleness}. These limitations hinder their deployment in high-stakes applications where robustness and verifiable reasoning are paramount.

To address these shortcomings, we introduce Agora, a configurable and reproducible framework for robust hierarchical classification. Our system is designed as a state-graph pipeline that decomposes the HMLC task into a series of manageable steps. Its core novelty lies in two mechanisms designed to enhance LLM reliability: (1) a multi-agent ensemble method where multiple independent LLM agents classify the text in parallel, with their outputs aggregated via a voting mechanism to form a robust consensus; and (2) an optional actor-critic self-refinement loop, where a dedicated ``critic'' agent validates the classifier's outputs and provides feedback for iterative correction.
